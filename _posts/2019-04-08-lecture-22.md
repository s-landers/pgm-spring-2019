---
layout: distill
title: "Lecture 22: Bayesian non-parametrics"
description: An introduction to Bayesian non-parametrics and the Dirichlet process.
date: 2019-04-08

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Yao Chong Lim  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yue Wu
    url: "#"
  - name: Dongshunyi Li
    url: "#"
  - name: Steve Landers
    url: "#"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "#"  # optional URL to the editor's homepage

---

## Motivation via Clustering

## The Dirichlet distribution

### Reason for examining the Dirichlet distribution

The Dirichlet distribution is a distribution over vectors with values between 0 and 1 that sum to 1, meaning these vectors can be used as the probabilities of a discrete distribution.  Because of this, the Dirichlet distribution can be seen as a distribution over distributions.

### Details of the Dirichlet distribution

The Dirichlet distribution is defined as

$$
\frac {\Pi^K_{k=1} \Gamma(\alpha_k)} {\Gamma(\Sigma^K_{k=1} \alpha_k)} \Pi^K_{k=1} \pi_k^{\alpha_k-1}
$$

where:

- $[\alpha_0 ... \alpha_k]$ are the parameters of the Dirichlet distribution.  $\alpha_k \geq 0$, and $\Sigma_k \alpha_k \gt 0$.
- $[\pi_0 ... \pi_k]$ are the values the Dirichlet distribution describes the probability of.  $\Sigma_k \pi_k = 1$, and can be seen as members of the $K-1$ standard simplex (generalized triangle).

### Properties of the Dirichlet distribution

- Probability is evenly distributed when $\alpha_k = 1$ for all $k$.
- $\alpha_k$ values less than 1 spread the probability away from the center.
- $\alpha_k$ values greater than 1 gather the probability in the center.
- If the value of $\alpha_i$ is greater than $\alpha_j$, probability will gather more towards vertex $j$ of the simplex.

<figure id="dir-alpha-same" class="l-body">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_0_1.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_0_25.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_0_5.png' | relative_url }}" />  
    </div>
  </div>
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_1.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_5.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_10.png' | relative_url }}" />  
    </div>
  </div>
  <figcaption>
      3D Dirichlet PDF for alpha values which are all the same.  Blue is zero, red is infinity.  X and Y are 1 at the right and top of the image, and Z is 1 at the bottom left.
    </figcaption>
</figure>

<figure id="dir-alpha-vertex" class="l-body">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_x_2.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_y_2.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_z_2.png' | relative_url }}" />  
    </div>
  </div>
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_x_50.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_y_50.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_z_50.png' | relative_url }}" />  
    </div>
  </div>
  <figcaption>
    If one alpha value is greater than another, probability will gather near the vertex of the simplex with the same index as the alpha value.
  </figcaption>
</figure>

<figure id="dir-alpha-cross" class="l-body">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_x_y_25.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_y_z_25.png' | relative_url }}" />  
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/alpha_x_z_25.png' | relative_url }}" />  
    </div>
  </div>
  <figcaption>
    Mixed alpha values allow probability peaks that straddle vertices.
  </figcaption>
</figure>

Expectation:

$$
\mathbb{E}\big[(\pi_1,...,\pi_k)\big] = \frac {(\alpha_1,...,\alpha_k)} {\Sigma_k \alpha_k}
$$

The Dirichlet distribution is the conjugate prior to the multinomial and categorical distributions.

### Dirichlet distribution as a distribution over distributions

Vectors sampled from the Dirichlet distribution can be used to represent distributions over the parameters of other distributions.  For example, each probability in a sampled vector could be associated with the mean and variance of a normal distribution, and that normal distribution could itself be used to produce the parameters of another distribution.  In this way, the Dirichlet distribution can act as a distribution over distributions of parameters.

### Operations on the Dirichlet distribution

The dimension of the Dirichlet distribution can be modified while maintaining some properties of the distributions it describes.

#### Collapsing

If $(\pi_1,...,\pi_k) \sim Dirichlet(\alpha_1,...,\alpha_k)$, then 

$(\pi_1 + \pi_2, \pi_3,...,\pi_k) \sim Dirichlet(\alpha_1 + \alpha_2, \alpha_3,...,\alpha_k)$

#### Splitting

If $(\pi_1,...,\pi_k) \sim Dirichlet(\alpha_1,...,\alpha_k)$ and 

$(\theta_1,...,\theta_m) \sim Dirichlet(\alpha_1 \beta_1,...,\alpha_1 \beta_m)$, then 

$(\pi_1 \theta_1,...,\pi_1 \theta_m, \pi_2,...,\pi_k) \sim Dirichlet(\alpha_1 \beta_1,...,\alpha_1 \beta_m, \alpha_2,...,\alpha_k)$

#### Renormalization

If $(\pi_1,...,\pi_k) \sim Dirichlet(\alpha_1,...,\alpha_k)$ then

$\frac {(\pi_2,...,\pi_k)} {\Sigma^K_{k=2} \pi_k} \sim Dirichlet(\alpha_2,...,\alpha_k)$

## Parametric vs nonparametric

## The Dirichlet process

## Metaphors for the Dirichlet process

There are many ways to visualize the Dirichlet process. Three common metaphors are the Polya urn scheme, the Chinese restaurant process, and the stick-breaking process.

### Pólya urn scheme

<figure id="polya-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/polya_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the Pólya urn scheme.
      </figcaption>
    </div>
  </div>
</figure>

<!-- The predictive distribution given by the Dirichlet process can be understood intuitively using the Polya urn scheme (also called the Blackwell-MacQueen urn scheme). -->

In the Pólya urn scheme (also called the Blackwell-MacQueen urn scheme)<d-cite key="blackwell1973"></d-cite>, imagine we have an urn initially containing a black ball of mass $\alpha$. To generate each sample $X_n$, sample a ball from the urn with probability proportional to its mass. The color of the ball is the partition that $X_n$ comes from. If the ball is black, choose a new previously-unseen color for the sample, and return the black ball and a unit-mass ball of that new color to the urn. If the ball is not black, just return it and another unit-mass ball of the same color to the urn.

Then, after sampling $n$ balls, the joint distribution of the balls' colors (i.e. the cluster the balls belong to) follows a Dirichlet process.

From this scheme, we can also easily see the self-reinforcing property that the Dirichlet process exhibits: new samples are more likely to be drawn from partitions that already have many existing samples.

### The Chinese restaurant process

<!-- [image?] -->

The Chinese restaurant process is an equivalent description of the Pólya urn scheme.

Imagine a restaurant with an infinite number of tables, all initially empty. The first customer enters the restaurant, and picks a table to sit at. When the $(n-1)$-th customer enters, they either 1) sit at an occupied table $k$ with probability $\frac{m_k}{n + \alpha}$, where $m_k$ is the number of people that are currently sitting at table $k$, or 2) start a new table with probability $\frac{\alpha}{n + \alpha}$. We can see that this description is equivalent to the Pólya urn scheme above.

<!-- We can generalize the Chinese restaurant process as follows: The probability now depends on both the number of people  -->

### Note on Exchangeability

Even though the two descriptions above have a notion of an _order_ of the drawn samples, it turns out that the distribution over the partitions of the first $N$ samples does not _depend on the order of the samples_. However this does not mean the samples are independent, since we have seen the self-reinforcing property of the Dirichlet process - samples tend to be drawn from partitions that already have many existing samples.

Distributions that do not depend on the order of the samples are called __exchangeable__ distributions.

De Finetti's theorem states that if a sequence of observations are exchangeable, then there must exist a distribution given which the samples are i.i.d. In this case, the samples are i.i.d. given the Dirichlet process.

### The stick-breaking process

<figure id="stick-breaking-gm" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-22/stick_breaking_gm.png' | relative_url }}" />
      <figcaption>
      Graphical model for the stick-breaking process.
      </figcaption>
    </div>
  </div>
</figure>

The two metaphors above primarily show the distribution of the data points among the resulting partitions. The stick-breaking process shows how the partitions of the DP can be constructed, along with their associated parameters, to give the parameters of the resulting distribution $G$<d-cite key="sethuraman1994"></d-cite>.

<!-- Another alternative view of the Dirichlet process is the stick-breaking process to construct the partitions of the DP and their associated parameters<d-cite key="sethuraman1994"></d-cite>. -->

Imagine we begin with a stick of unit length, which represents our total probability. We then repeatedly break off fractions of the stick, and assign parameters to each broken-off fraction according to our base measure.

Concretely, to construct the $k$-th partition:
1. Sample a Beta$(1, \alpha)$ random variable $\beta_k \in [0, 1]$.
1. Break off a fraction $\beta_k$ of the remaining stick. This gives us the $k$-th partition. We can calculate its atom size $\pi_k$ using the previous fractions $\beta_1, ..., \beta_k$.
1. Sample a random parameter $\theta_k$ for the partition for this atom from our base measure $G_0$.
1. Recur on the remaining stick.

In summary,
<d-math block>
\begin{aligned}
\beta_k &\sim \text{Beta}(1, \alpha) \\
\pi_k &= \beta_k \prod_{\ell=1}^{k-1} (1 - \beta_{\ell}) \\
\theta_k &\sim G_0 \\
G &= \sum_{k=1}^{\infty} \pi_k \delta_{\theta_k} \sim \text{DP}(\alpha, G_0)
\end{aligned}
</d-math>

The stick-breaking metaphor shows us how we can approximate a Dirichlet process. The final distribution $G$ is obtained from a weighted sum of the sampled parameters $\theta_k$, weighted by the atom size $\pi_k$. Since the atom sizes of the created partitions in the stick-breaking process are monotonically decreasing, we can get a good approximation of $G$ by just using the first $k$ sampled parameters and atom sizes.

